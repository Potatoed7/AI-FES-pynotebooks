{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Supervised Learning: Classification with sklearn\n","\n","Let's take a look at some supervised learning examples using sklearn. We'll start with some image classification examples, followed by a look at linear regression. However, one important point: your choice of classification model matters greatly. Different models will excel at different tasks. You can see a comparison of classifiers here:\n","\n","![](https://scikit-learn.org/stable/_images/sphx_glr_plot_classifier_comparison_001.png)\n","\n","We'll be looking at a few options, but we don't have nearly enough time to cover the details of all. For now, explore and test!"],"metadata":{"id":"26_cYhBMnpaz"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn import metrics"],"metadata":{"id":"YxcWVz6j3Buc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We'll import our dataset using the keras library. You might recall Keras is another python machine learning library. We're only using it to easily obtain the dataset here; we'll still be doing all our training using sklearn."],"metadata":{"id":"6xZeqBaNBrzX"}},{"cell_type":"code","source":["import keras\n","\n","(X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data() #returns a tuple of numpy arrays\n","\n","#X_train: NumPy array of grayscale image data with shapes (60000, 28, 28), containing the training data.\n","#y_train: NumPy array of labels (integers in range 0-9) with shape (60000,) for the training data.\n","#X_test: NumPy array of grayscale image data with shapes (10000, 28, 28), containing the test data.\n","#y_test: NumPy array of labels (integers in range 0-9) with shape (10000,) for the test data."],"metadata":{"id":"pkSENV41QW7w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# For model training speed purposes, we'll cut out the majority of our dataset\n","X_train = X_train[:1200] #keep only the first 1200 images\n","y_train = y_train[:1200] #keep only the first 1200 labels\n","X_test = X_test[:200] #keep only the first 200 images\n","y_test = y_test[:200] #keep only the first 200 labels"],"metadata":{"id":"hreOdqyNk4lv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train[0]"],"metadata":{"id":"unDmEtZgRagO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In the dataset, the labels are classified by number:\n","\n","*    0 = T-shirt/top\n","*    1 = Trouser\n","*    2 = Pullover\n","*    3 = Dress\n","*    4 = Coat\n","*    5 = Sandal\n","*    6 = Shirt\n","*    7 = Sneaker\n","*    8 = Bag\n","*    9 = Ankle boot"],"metadata":{"id":"wSt8fn72VVWq"}},{"cell_type":"code","source":["y_train[0] #this is the label, i.e. the classification, of X_train[0]"],"metadata":{"id":"QO6c3L40VKd6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can take a quick look at a subset of images in the dataset by plotting them with matplotlib:"],"metadata":{"id":"aLNZWJ3zCeoy"}},{"cell_type":"code","source":["n_row = 1\n","n_col = 5\n","plt.figure(figsize=(10,8))\n","for i in list(range(n_row * n_col)):\n","    plt.subplot(n_row, n_col, i+1)\n","    plt.imshow(X_train[i,:].reshape(28,28), cmap=\"gray\")\n","    title_text = \"Image\" + str(i+1)\n","    plt.title(title_text, size=6.5)\n","\n","plt.show()"],"metadata":{"id":"K16EewDlQgNk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["One important thing we'll need to do in order to prepare our training data, is reduce the dimensionality of our arrays. Currently, they are three dimensional, but models need them to be 2 dimensional."],"metadata":{"id":"L0tVowIhCkOg"}},{"cell_type":"code","source":["print(X_train.shape)\n","print(X_test.shape)"],"metadata":{"id":"qQgNMl9bCr6Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To fix this, we can use the [.reshape() method](https://numpy.org/doc/stable/reference/generated/numpy.reshape.html) to flatten our 28 x 28 image data:"],"metadata":{"id":"VdR04oamEoBO"}},{"cell_type":"code","source":["nsamples, nx, ny = X_train.shape\n","X_train_d2 = X_train.reshape((nsamples, nx * ny))\n","nsamples, nx, ny = X_test.shape\n","X_test_d2 = X_test.reshape((nsamples, nx * ny))"],"metadata":{"id":"BgbTbN7xTNd6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(X_train_d2.shape)\n","print(X_test_d2.shape)"],"metadata":{"id":"9uqhIHBRFBBv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now that we've prepped our training data, we can train a model. We'll start with a [MLP Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html). As we discussed earlier, this can work for simple images but struggles with high-res, complex images. But our images here are quite simple, so let's see how it does:"],"metadata":{"id":"zEVIaB-a0ThQ"}},{"cell_type":"code","source":["from sklearn.neural_network import MLPClassifier\n","\n","MLP_model = MLPClassifier(max_iter=500, tol=1e-3)\n","MLP_model.fit(X_train_d2,y_train)\n","mlp_predict = MLP_model.predict(X_test_d2)"],"metadata":{"id":"GrTxlqyL28CF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(metrics.classification_report(y_test, mlp_predict))\n","print(\"average accuracy:\", np.mean(y_test == mlp_predict) * 100)"],"metadata":{"id":"N0e211w6CAFl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next let's try a [Logistic Regression classifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) (logistic regression was traditionally designed for binary classifications but has been improved in sklearn to support multi-class classification):"],"metadata":{"id":"ptBECBCi240w"}},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","lr_model = LogisticRegression(multi_class=\"multinomial\",\n","                                    solver=\"saga\", max_iter=100, tol=1e-3)\n","lr_model.fit(X_train_d2, y_train)\n","lr_predict = lr_model.predict(X_test_d2)"],"metadata":{"id":"SMBzIOiM0mlN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(metrics.classification_report(y_test, lr_predict))\n","print(\"average accuracy:\", np.mean(y_test == lr_predict) * 100)"],"metadata":{"id":"3iR38fF20mn-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can try other models, and you can explore on your own as well - simply import the model and take a quick look at the documentation to read up on the parameters to see if any need to be specified."],"metadata":{"id":"5StTCXd76SU8"}},{"cell_type":"code","source":["from sklearn.naive_bayes import GaussianNB\n","\n","gNB_model = GaussianNB()\n","gNB_model.fit(X_train_d2,y_train)\n","nb_predict = gNB_model.predict(X_test_d2)\n","\n","print(metrics.classification_report(y_test, nb_predict))\n","print(\"average accuracy:\", np.mean(y_test == nb_predict) * 100)"],"metadata":{"id":"sQZFZDwu0mrE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.svm import SVC\n","\n","svm_model = SVC(max_iter=100, tol=1e-3)\n","svm_model.fit(X_train_d2,y_train)\n","svm_predict = svm_model.predict(X_test_d2)\n","\n","print(metrics.classification_report(y_test, svm_predict))\n","print(\"average accuracy:\", np.mean(y_test == svm_predict) * 100)"],"metadata":{"id":"nVuSLtR851H4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"8HLB_gIk51KW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Vn1nRlDh51NT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Principal Component Analysis (PCA)\n","\n","**Principal Component Analysis** (PCA) is a dimensionality reduction technique used to simplify the complexity of high-dimensional data while preserving most of its important features. It achieves this by transforming the original features into a new set variables called principal components. This can help eliminate redundancy. For example if 10 out of 12 variables all measure similar things, they might be given too much weight. e.g.\n","*    variable1 = temperature\n","*    variable2 = humidity\n","*    variable3 = wind speed\n","\n","These variables might all be reduced to one feature called weather.\n","\n","\n"],"metadata":{"id":"QNaSUCEKkZTn"}},{"cell_type":"markdown","source":["Note that Principal Component Analysis (PCA) can potentially reduce accuracy in some cases because some information may be lost, especially if the new principal components do not capture all the variation in the original data. Likewise, there is a risk that the principal components capture noise rather than signal, resulting in a loss of accuracy.\n","\n","*    **noise**: irrelevant or random variations in the data that do not represent meaningful patterns or relationships\n","*    **signal**: meaningful patterns in the data that is relevant to the task at hand\n","\n","It also assumes a linear relationship between variables - if the relationship between variables is non-linear, PCA may struggle to properly capture the relationships between variables."],"metadata":{"id":"d4P4W3mQk3fP"}},{"cell_type":"code","source":["from sklearn.decomposition import PCA\n","\n","#let's redownload the full 60,000 row dataset and use PCA\n","(X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n","nsamples, nx, ny = X_train.shape\n","X_train_d2 = X_train.reshape((nsamples, nx * ny))\n","nsamples, nx, ny = X_test.shape\n","X_test_d2 = X_test.reshape((nsamples, nx * ny))\n","\n","#specify the number of principal components to retain\n","n_components = 400\n","pca = PCA(n_components=n_components)\n","X_train_pca = pca.fit_transform(X_train_d2)\n","X_test_pca = pca.fit_transform(X_test_d2)"],"metadata":{"id":"p2DDYJOEP80r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","lr_model = LogisticRegression(multi_class=\"multinomial\",\n","                                    solver=\"saga\", max_iter=100, tol=1e-3)\n","lr_model.fit(X_train_d2, y_train)\n","lr_predict = lr_model.predict(X_test_d2)\n","\n","print(metrics.classification_report(y_test, lr_predict))\n","print(\"average accuracy:\", np.mean(y_test == lr_predict) * 100)"],"metadata":{"id":"sAiG_hRyBp9l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"uGsD896s1z2J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"2lmmR-HI1z4j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercise: Training a Classification Model\n","\n","We just learned how to do classification using Fashion MNIST, a data set containing items of clothing. There's another, similar dataset called MNIST which has images of handwriting -- specifically handwritten digits 0 through 9.\n","\n","*    Write an MNIST classifier that is trained to recognise the written digit. I've started the code for you below -- how would you finish it? What's the best accuracy you can achieve?"],"metadata":{"id":"k9fIubWIvNxQ"}},{"cell_type":"code","source":["import tensorflow as tf\n","mnist = tf.keras.datasets.mnist\n","\n","(X_train, y_train),(X_test, y_test) = mnist.load_data()"],"metadata":{"id":"BviWe77t1z6y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train[0]"],"metadata":{"id":"84RkmZy1wT0M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_train[0] #once again, this is the label, i.e. classification, of X_train[0]"],"metadata":{"id":"ExuZ8OxK65tY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nsamples, nx, ny = X_train.shape\n","X_train_d2 = X_train.reshape((nsamples, nx * ny))\n","nsamples, nx, ny = X_test.shape\n","X_test_d2 = X_test.reshape((nsamples, nx * ny))"],"metadata":{"id":"roJF4tSLxpYG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"VQKspXiQ8rDJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ILcxSApQ8rDJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"sh4bkJ5o9Ish"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"bNndiutM9Isi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"s_RGyTtG1z81"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"GMMmXiLUBWmz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7b-qxajA0FmX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"dAuDoH0z0FrY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Uy4avYks0FuB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Wi9Aa7Cj10Bl"},"execution_count":null,"outputs":[]}]}