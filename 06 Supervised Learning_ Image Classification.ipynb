{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Supervised Learning: Classification with sklearn\n",
        "\n",
        "Let's take a look at some supervised learning examples using sklearn. We'll start with some image classification examples, followed by a look at linear regression. However, one important point: your choice of classification model matters greatly. Different models will excel at different tasks. You can see a comparison of classifiers here:\n",
        "\n",
        "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_classifier_comparison_001.png)\n",
        "\n",
        "We'll be looking at a few options, but we don't have nearly enough time to cover the details of all. For now, explore and test!"
      ],
      "metadata": {
        "id": "26_cYhBMnpaz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics"
      ],
      "metadata": {
        "id": "YxcWVz6j3Buc"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll import our dataset using the keras library. You might recall Keras is another python machine learning library. We're only using it to easily obtain the dataset here; we'll still be doing all our training using sklearn."
      ],
      "metadata": {
        "id": "6xZeqBaNBrzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data() #returns a tuple of numpy arrays\n",
        "\n",
        "#X_train: NumPy array of grayscale image data with shapes (60000, 28, 28), containing the training data.\n",
        "#y_train: NumPy array of labels (integers in range 0-9) with shape (60000,) for the training data.\n",
        "#X_test: NumPy array of grayscale image data with shapes (10000, 28, 28), containing the test data.\n",
        "#y_test: NumPy array of labels (integers in range 0-9) with shape (10000,) for the test data."
      ],
      "metadata": {
        "id": "pkSENV41QW7w",
        "outputId": "1760d450-b5f9-4fbf-be64-4861a595ed46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "wMidXNNdPPZQ",
        "outputId": "441d179e-9806-4f07-dcc8-84018c9003dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "metadata": {
        "id": "U3HweT8aPb99",
        "outputId": "33d8f286-e2ba-4a19-9518-efe82ab452ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000,)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For model training speed purposes, we'll cut out the majority of our dataset\n",
        "X_train = X_train[:1200] #keep only the first 1200 images\n",
        "y_train = y_train[:1200] #keep only the first 1200 labels\n",
        "X_test = X_test[:200] #keep only the first 200 images\n",
        "y_test = y_test[:200] #keep only the first 200 labels"
      ],
      "metadata": {
        "id": "hreOdqyNk4lv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0] #print first object"
      ],
      "metadata": {
        "id": "unDmEtZgRagO",
        "outputId": "0180a37c-252b-4dbe-8b88-cda11d50b4dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,\n",
              "          0,   0,  13,  73,   0,   0,   1,   4,   0,   0,   0,   0,   1,\n",
              "          1,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
              "          0,  36, 136, 127,  62,  54,   0,   0,   0,   1,   3,   4,   0,\n",
              "          0,   3],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   6,\n",
              "          0, 102, 204, 176, 134, 144, 123,  23,   0,   0,   0,   0,  12,\n",
              "         10,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0, 155, 236, 207, 178, 107, 156, 161, 109,  64,  23,  77, 130,\n",
              "         72,  15],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   0,\n",
              "         69, 207, 223, 218, 216, 216, 163, 127, 121, 122, 146, 141,  88,\n",
              "        172,  66],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   1,   1,   0,\n",
              "        200, 232, 232, 233, 229, 223, 223, 215, 213, 164, 127, 123, 196,\n",
              "        229,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "        183, 225, 216, 223, 228, 235, 227, 224, 222, 224, 221, 223, 245,\n",
              "        173,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "        193, 228, 218, 213, 198, 180, 212, 210, 211, 213, 223, 220, 243,\n",
              "        202,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   3,   0,  12,\n",
              "        219, 220, 212, 218, 192, 169, 227, 208, 218, 224, 212, 226, 197,\n",
              "        209,  52],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   6,   0,  99,\n",
              "        244, 222, 220, 218, 203, 198, 221, 215, 213, 222, 220, 245, 119,\n",
              "        167,  56],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   4,   0,   0,  55,\n",
              "        236, 228, 230, 228, 240, 232, 213, 218, 223, 234, 217, 217, 209,\n",
              "         92,   0],\n",
              "       [  0,   0,   1,   4,   6,   7,   2,   0,   0,   0,   0,   0, 237,\n",
              "        226, 217, 223, 222, 219, 222, 221, 216, 223, 229, 215, 218, 255,\n",
              "         77,   0],\n",
              "       [  0,   3,   0,   0,   0,   0,   0,   0,   0,  62, 145, 204, 228,\n",
              "        207, 213, 221, 218, 208, 211, 218, 224, 223, 219, 215, 224, 244,\n",
              "        159,   0],\n",
              "       [  0,   0,   0,   0,  18,  44,  82, 107, 189, 228, 220, 222, 217,\n",
              "        226, 200, 205, 211, 230, 224, 234, 176, 188, 250, 248, 233, 238,\n",
              "        215,   0],\n",
              "       [  0,  57, 187, 208, 224, 221, 224, 208, 204, 214, 208, 209, 200,\n",
              "        159, 245, 193, 206, 223, 255, 255, 221, 234, 221, 211, 220, 232,\n",
              "        246,   0],\n",
              "       [  3, 202, 228, 224, 221, 211, 211, 214, 205, 205, 205, 220, 240,\n",
              "         80, 150, 255, 229, 221, 188, 154, 191, 210, 204, 209, 222, 228,\n",
              "        225,   0],\n",
              "       [ 98, 233, 198, 210, 222, 229, 229, 234, 249, 220, 194, 215, 217,\n",
              "        241,  65,  73, 106, 117, 168, 219, 221, 215, 217, 223, 223, 224,\n",
              "        229,  29],\n",
              "       [ 75, 204, 212, 204, 193, 205, 211, 225, 216, 185, 197, 206, 198,\n",
              "        213, 240, 195, 227, 245, 239, 223, 218, 212, 209, 222, 220, 221,\n",
              "        230,  67],\n",
              "       [ 48, 203, 183, 194, 213, 197, 185, 190, 194, 192, 202, 214, 219,\n",
              "        221, 220, 236, 225, 216, 199, 206, 186, 181, 177, 172, 181, 205,\n",
              "        206, 115],\n",
              "       [  0, 122, 219, 193, 179, 171, 183, 196, 204, 210, 213, 207, 211,\n",
              "        210, 200, 196, 194, 191, 195, 191, 198, 192, 176, 156, 167, 177,\n",
              "        210,  92],\n",
              "       [  0,   0,  74, 189, 212, 191, 175, 172, 175, 181, 185, 188, 189,\n",
              "        188, 193, 198, 204, 209, 210, 210, 211, 188, 188, 194, 192, 216,\n",
              "        170,   0],\n",
              "       [  2,   0,   0,   0,  66, 200, 222, 237, 239, 242, 246, 243, 244,\n",
              "        221, 220, 193, 191, 179, 182, 182, 181, 176, 166, 168,  99,  58,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  40,  61,  44,  72,  41,  35,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]], dtype=uint8)"
            ],
            "text/html": [
              "<style>\n",
              "      .ndarray_repr .ndarray_raw_data {\n",
              "        display: none;\n",
              "      }\n",
              "      .ndarray_repr.show_array .ndarray_raw_data {\n",
              "        display: block;\n",
              "      }\n",
              "      .ndarray_repr.show_array .ndarray_image_preview {\n",
              "        display: none;\n",
              "      }\n",
              "      </style>\n",
              "      <div id=\"id-b567e1e8-2c54-4105-8de1-b5264fa7f775\" class=\"ndarray_repr\"><pre>ndarray (28, 28) <button style=\"padding: 0 2px;\">show data</button></pre><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACBUlEQVR4nLXSz0tUURQH8O+597373rx545g6NmNJY6TlJgozEsKFZGCrkDBoVdGmdf9BiwJ3bVr1PwhBUbQfJQcKjWZRTsjo0KDpDPV+eN+997WK0Z2bvtsPfM+Bc4D/HwJycwBZAEAE6zAyfe5RFMQfFYiRAj+CXM/c2HK82VetVMM34RGUmCxz9v7yYnW9dnWyskyHR6azi72Jwep3ScVk9c7LLhKQrpRBSiI2n76puZFT3doUwH4pcmw/zpjrU2zw3dFt4XEWdvbKhpinzTBYt5bDH4qlLYO8cbKBWHOudDHluFtiLrLD0kmM2//6q9VFS+JLLDxv9GzMPV9v3XuzYgHEGSUGCngbRCLd4W6CxPCLHTDwVMkDA0y/qEa/lFFBoF2EEME8CAD6hsZK8+djlmSatuiXXsWfNh27NU6Yelro1bytPElRbaGaO1FGPfc7zPg9HvHlIaVD8AjID9y/+bgZ/6iP9ks7J/QZevh8w/cd2PlGs8CKt92R7MQEk0yA7GtWq9ETN3zRs7fpR7FaWi/3yXaipW1IjFnbaSM70N7dsRzbzbHd8aCx7+wmKskUO5esz0sPmvXYF67gBzoNfxptxb5stxM10iLg1pOTO23NhcUptW1hE6gFYYprC8QMMPNsMM+4pamVbps/HGkSsg+1Cv4d+0Jh//Sm3DjGix4rfwFoJNh2/0cDFgAAAABJRU5ErkJggg==\" class=\"ndarray_image_preview\" /><pre class=\"ndarray_raw_data\">array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,\n",
              "          0,   0,  13,  73,   0,   0,   1,   4,   0,   0,   0,   0,   1,\n",
              "          1,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
              "          0,  36, 136, 127,  62,  54,   0,   0,   0,   1,   3,   4,   0,\n",
              "          0,   3],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   6,\n",
              "          0, 102, 204, 176, 134, 144, 123,  23,   0,   0,   0,   0,  12,\n",
              "         10,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0, 155, 236, 207, 178, 107, 156, 161, 109,  64,  23,  77, 130,\n",
              "         72,  15],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   0,\n",
              "         69, 207, 223, 218, 216, 216, 163, 127, 121, 122, 146, 141,  88,\n",
              "        172,  66],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   1,   1,   0,\n",
              "        200, 232, 232, 233, 229, 223, 223, 215, 213, 164, 127, 123, 196,\n",
              "        229,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "        183, 225, 216, 223, 228, 235, 227, 224, 222, 224, 221, 223, 245,\n",
              "        173,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "        193, 228, 218, 213, 198, 180, 212, 210, 211, 213, 223, 220, 243,\n",
              "        202,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   3,   0,  12,\n",
              "        219, 220, 212, 218, 192, 169, 227, 208, 218, 224, 212, 226, 197,\n",
              "        209,  52],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   6,   0,  99,\n",
              "        244, 222, 220, 218, 203, 198, 221, 215, 213, 222, 220, 245, 119,\n",
              "        167,  56],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   4,   0,   0,  55,\n",
              "        236, 228, 230, 228, 240, 232, 213, 218, 223, 234, 217, 217, 209,\n",
              "         92,   0],\n",
              "       [  0,   0,   1,   4,   6,   7,   2,   0,   0,   0,   0,   0, 237,\n",
              "        226, 217, 223, 222, 219, 222, 221, 216, 223, 229, 215, 218, 255,\n",
              "         77,   0],\n",
              "       [  0,   3,   0,   0,   0,   0,   0,   0,   0,  62, 145, 204, 228,\n",
              "        207, 213, 221, 218, 208, 211, 218, 224, 223, 219, 215, 224, 244,\n",
              "        159,   0],\n",
              "       [  0,   0,   0,   0,  18,  44,  82, 107, 189, 228, 220, 222, 217,\n",
              "        226, 200, 205, 211, 230, 224, 234, 176, 188, 250, 248, 233, 238,\n",
              "        215,   0],\n",
              "       [  0,  57, 187, 208, 224, 221, 224, 208, 204, 214, 208, 209, 200,\n",
              "        159, 245, 193, 206, 223, 255, 255, 221, 234, 221, 211, 220, 232,\n",
              "        246,   0],\n",
              "       [  3, 202, 228, 224, 221, 211, 211, 214, 205, 205, 205, 220, 240,\n",
              "         80, 150, 255, 229, 221, 188, 154, 191, 210, 204, 209, 222, 228,\n",
              "        225,   0],\n",
              "       [ 98, 233, 198, 210, 222, 229, 229, 234, 249, 220, 194, 215, 217,\n",
              "        241,  65,  73, 106, 117, 168, 219, 221, 215, 217, 223, 223, 224,\n",
              "        229,  29],\n",
              "       [ 75, 204, 212, 204, 193, 205, 211, 225, 216, 185, 197, 206, 198,\n",
              "        213, 240, 195, 227, 245, 239, 223, 218, 212, 209, 222, 220, 221,\n",
              "        230,  67],\n",
              "       [ 48, 203, 183, 194, 213, 197, 185, 190, 194, 192, 202, 214, 219,\n",
              "        221, 220, 236, 225, 216, 199, 206, 186, 181, 177, 172, 181, 205,\n",
              "        206, 115],\n",
              "       [  0, 122, 219, 193, 179, 171, 183, 196, 204, 210, 213, 207, 211,\n",
              "        210, 200, 196, 194, 191, 195, 191, 198, 192, 176, 156, 167, 177,\n",
              "        210,  92],\n",
              "       [  0,   0,  74, 189, 212, 191, 175, 172, 175, 181, 185, 188, 189,\n",
              "        188, 193, 198, 204, 209, 210, 210, 211, 188, 188, 194, 192, 216,\n",
              "        170,   0],\n",
              "       [  2,   0,   0,   0,  66, 200, 222, 237, 239, 242, 246, 243, 244,\n",
              "        221, 220, 193, 191, 179, 182, 182, 181, 176, 166, 168,  99,  58,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  40,  61,  44,  72,  41,  35,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]], dtype=uint8)</pre></div><script>\n",
              "      (() => {\n",
              "      const titles = ['show data', 'hide data'];\n",
              "      let index = 0\n",
              "      document.querySelector('#id-b567e1e8-2c54-4105-8de1-b5264fa7f775 button').onclick = (e) => {\n",
              "        document.querySelector('#id-b567e1e8-2c54-4105-8de1-b5264fa7f775').classList.toggle('show_array');\n",
              "        index = (++index) % 2;\n",
              "        document.querySelector('#id-b567e1e8-2c54-4105-8de1-b5264fa7f775 button').textContent = titles[index];\n",
              "        e.preventDefault();\n",
              "        e.stopPropagation();\n",
              "      }\n",
              "      })();\n",
              "    </script>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the dataset, the labels are classified by number:\n",
        "\n",
        "*    0 = T-shirt/top\n",
        "*    1 = Trouser\n",
        "*    2 = Pullover\n",
        "*    3 = Dress\n",
        "*    4 = Coat\n",
        "*    5 = Sandal\n",
        "*    6 = Shirt\n",
        "*    7 = Sneaker\n",
        "*    8 = Bag\n",
        "*    9 = Ankle boot"
      ],
      "metadata": {
        "id": "wSt8fn72VVWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[0] #this is the label, i.e. the classification, of X_train[0]"
      ],
      "metadata": {
        "id": "QO6c3L40VKd6",
        "outputId": "98924960-9bd9-4bb8-eada-224403ae49e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can take a quick look at a subset of images in the dataset by plotting them with matplotlib:"
      ],
      "metadata": {
        "id": "aLNZWJ3zCeoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_row = 1\n",
        "n_col = 5\n",
        "plt.figure(figsize=(10,8))\n",
        "for i in list(range(n_row * n_col)):\n",
        "    plt.subplot(n_row, n_col, i+1)\n",
        "    plt.imshow(X_train[i,:].reshape(28,28), cmap=\"gray\")\n",
        "    title_text = \"Image\" + str(i+1)\n",
        "    plt.title(title_text, size=6.5)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "K16EewDlQgNk",
        "outputId": "97424645-7ee6-4e97-f410-af3168c5d8d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One important thing we'll need to do in order to prepare our training data, is reduce the dimensionality of our arrays. Currently, they are three dimensional, but models need them to be 2 dimensional."
      ],
      "metadata": {
        "id": "L0tVowIhCkOg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "id": "qQgNMl9bCr6Q",
        "outputId": "79db5bd2-540d-4660-b6fe-3eeb6a709222",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1200, 28, 28)\n",
            "(200, 28, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To fix this, we can use the [.reshape() method](https://numpy.org/doc/stable/reference/generated/numpy.reshape.html) to flatten our 28 x 28 image data:"
      ],
      "metadata": {
        "id": "VdR04oamEoBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nsamples, nx, ny = X_train.shape\n",
        "X_train_d2 = X_train.reshape((nsamples, nx * ny))\n",
        "nsamples, nx, ny = X_test.shape\n",
        "X_test_d2 = X_test.reshape((nsamples, nx * ny))\\\n",
        "#makes 1200 x 28 x 28 ---> 1200 x 784"
      ],
      "metadata": {
        "id": "BgbTbN7xTNd6"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train_d2.shape)\n",
        "print(X_test_d2.shape)"
      ],
      "metadata": {
        "id": "9uqhIHBRFBBv",
        "outputId": "b5eb9400-36b3-4614-9631-d0ae64df2caf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1200, 784)\n",
            "(200, 784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've prepped our training data, we can train a model. We'll start with a [MLP Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html). As we discussed earlier, this can work for simple images but struggles with high-res, complex images. But our images here are quite simple, so let's see how it does:"
      ],
      "metadata": {
        "id": "zEVIaB-a0ThQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "MLP_model = MLPClassifier(max_iter=5090, tol=1e-5) #choose eour model , set the parameters\n",
        "MLP_model.fit(X_train_d2,y_train)    #train the model\n",
        "mlp_predict = MLP_model.predict(X_test_d2)   #try to predict"
      ],
      "metadata": {
        "id": "GrTxlqyL28CF"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics.classification_report(y_test, mlp_predict))\n",
        "print(\"average accuracy:\", np.mean(y_test == mlp_predict) * 100)"
      ],
      "metadata": {
        "id": "N0e211w6CAFl",
        "outputId": "de4a7102-ab25-4a3e-8732-57120c9bbc49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.85      0.77        20\n",
            "           1       1.00      0.96      0.98        27\n",
            "           2       0.74      0.63      0.68        27\n",
            "           3       0.63      0.71      0.67        17\n",
            "           4       0.65      0.81      0.72        21\n",
            "           5       0.82      0.88      0.85        16\n",
            "           6       0.58      0.44      0.50        16\n",
            "           7       0.80      0.80      0.80        20\n",
            "           8       0.94      0.83      0.88        18\n",
            "           9       0.76      0.72      0.74        18\n",
            "\n",
            "    accuracy                           0.77       200\n",
            "   macro avg       0.76      0.76      0.76       200\n",
            "weighted avg       0.77      0.77      0.77       200\n",
            "\n",
            "average accuracy: 77.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next let's try a [Logistic Regression classifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) (logistic regression was traditionally designed for binary classifications but has been improved in sklearn to support multi-class classification):"
      ],
      "metadata": {
        "id": "ptBECBCi240w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "lr_model = LogisticRegression(multi_class=\"multinomial\",\n",
        "                                    solver=\"saga\", max_iter=100, tol=1e-3)\n",
        "lr_model.fit(X_train_d2, y_train)\n",
        "lr_predict = lr_model.predict(X_test_d2)"
      ],
      "metadata": {
        "id": "SMBzIOiM0mlN",
        "outputId": "efd6810a-7f8b-4633-aadc-e8ba279ba59c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics.classification_report(y_test, lr_predict))\n",
        "print(\"average accuracy:\", np.mean(y_test == lr_predict) * 100)"
      ],
      "metadata": {
        "id": "3iR38fF20mn-",
        "outputId": "1e249e6e-3795-4c69-9afa-bca76765b32e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.90      0.92        20\n",
            "           1       0.96      1.00      0.98        27\n",
            "           2       0.82      0.67      0.73        27\n",
            "           3       0.81      0.76      0.79        17\n",
            "           4       0.64      0.67      0.65        21\n",
            "           5       0.93      0.81      0.87        16\n",
            "           6       0.48      0.69      0.56        16\n",
            "           7       0.82      0.90      0.86        20\n",
            "           8       1.00      0.89      0.94        18\n",
            "           9       0.89      0.89      0.89        18\n",
            "\n",
            "    accuracy                           0.82       200\n",
            "   macro avg       0.83      0.82      0.82       200\n",
            "weighted avg       0.84      0.82      0.82       200\n",
            "\n",
            "average accuracy: 82.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can try other models, and you can explore on your own as well - simply import the model and take a quick look at the documentation to read up on the parameters to see if any need to be specified."
      ],
      "metadata": {
        "id": "5StTCXd76SU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "gNB_model = GaussianNB()\n",
        "gNB_model.fit(X_train_d2,y_train)\n",
        "nb_predict = gNB_model.predict(X_test_d2)\n",
        "\n",
        "print(metrics.classification_report(y_test, nb_predict))\n",
        "print(\"average accuracy:\", np.mean(y_test == nb_predict) * 100)"
      ],
      "metadata": {
        "id": "sQZFZDwu0mrE",
        "outputId": "f71ffd3a-3034-4867-e097-efdcea80b403",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.75      0.81        20\n",
            "           1       0.83      0.56      0.67        27\n",
            "           2       0.56      0.37      0.44        27\n",
            "           3       0.43      0.94      0.59        17\n",
            "           4       0.43      0.71      0.54        21\n",
            "           5       0.50      0.19      0.27        16\n",
            "           6       0.20      0.06      0.10        16\n",
            "           7       0.49      0.90      0.63        20\n",
            "           8       0.88      0.83      0.86        18\n",
            "           9       1.00      0.56      0.71        18\n",
            "\n",
            "    accuracy                           0.59       200\n",
            "   macro avg       0.62      0.59      0.56       200\n",
            "weighted avg       0.63      0.59      0.57       200\n",
            "\n",
            "average accuracy: 59.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "svm_model = SVC(max_iter=100, tol=1e-3)\n",
        "svm_model.fit(X_train_d2,y_train)\n",
        "svm_predict = svm_model.predict(X_test_d2)\n",
        "\n",
        "print(metrics.classification_report(y_test, svm_predict))\n",
        "print(\"average accuracy:\", np.mean(y_test == svm_predict) * 100)"
      ],
      "metadata": {
        "id": "nVuSLtR851H4",
        "outputId": "135f86b7-0847-4601-a14e-0fb3576a8878",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.90      0.92        20\n",
            "           1       1.00      1.00      1.00        27\n",
            "           2       0.75      0.67      0.71        27\n",
            "           3       0.85      1.00      0.92        17\n",
            "           4       0.67      0.67      0.67        21\n",
            "           5       0.88      0.94      0.91        16\n",
            "           6       0.59      0.62      0.61        16\n",
            "           7       0.85      0.85      0.85        20\n",
            "           8       0.94      0.94      0.94        18\n",
            "           9       0.94      0.89      0.91        18\n",
            "\n",
            "    accuracy                           0.84       200\n",
            "   macro avg       0.84      0.85      0.84       200\n",
            "weighted avg       0.85      0.84      0.84       200\n",
            "\n",
            "average accuracy: 84.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8HLB_gIk51KW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vn1nRlDh51NT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Principal Component Analysis (PCA)\n",
        "\n",
        "**Principal Component Analysis** (PCA) is a dimensionality reduction technique used to simplify the complexity of high-dimensional data while preserving most of its important features. It achieves this by transforming the original features into a new set variables called principal components. This can help eliminate redundancy. For example if 10 out of 12 variables all measure similar things, they might be given too much weight. e.g.\n",
        "*    variable1 = temperature\n",
        "*    variable2 = humidity\n",
        "*    variable3 = wind speed\n",
        "\n",
        "These variables might all be reduced to one feature called weather.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QNaSUCEKkZTn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that Principal Component Analysis (PCA) can potentially reduce accuracy in some cases because some information may be lost, especially if the new principal components do not capture all the variation in the original data. Likewise, there is a risk that the principal components capture noise rather than signal, resulting in a loss of accuracy.\n",
        "\n",
        "*    **noise**: irrelevant or random variations in the data that do not represent meaningful patterns or relationships\n",
        "*    **signal**: meaningful patterns in the data that is relevant to the task at hand\n",
        "\n",
        "It also assumes a linear relationship between variables - if the relationship between variables is non-linear, PCA may struggle to properly capture the relationships between variables."
      ],
      "metadata": {
        "id": "d4P4W3mQk3fP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "#let's redownload the full 60,000 row dataset and use PCA\n",
        "(X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "nsamples, nx, ny = X_train.shape\n",
        "X_train_d2 = X_train.reshape((nsamples, nx * ny))\n",
        "nsamples, nx, ny = X_test.shape\n",
        "X_test_d2 = X_test.reshape((nsamples, nx * ny))\n",
        "\n",
        "#specify the number of principal components to retain\n",
        "n_components = 400\n",
        "pca = PCA(n_components=n_components)\n",
        "X_train_pca = pca.fit_transform(X_train_d2)\n",
        "X_test_pca = pca.fit_transform(X_test_d2)"
      ],
      "metadata": {
        "id": "p2DDYJOEP80r"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "lr_model = LogisticRegression(multi_class=\"multinomial\",\n",
        "                                    solver=\"saga\", max_iter=100, tol=1e-3)\n",
        "lr_model.fit(X_train_d2, y_train)\n",
        "lr_predict = lr_model.predict(X_test_d2)\n",
        "\n",
        "print(metrics.classification_report(y_test, lr_predict))\n",
        "print(\"average accuracy:\", np.mean(y_test == lr_predict) * 100)"
      ],
      "metadata": {
        "id": "sAiG_hRyBp9l",
        "outputId": "5eea22e7-1171-4cbb-f845-443b2c67b6fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.80      0.80      1000\n",
            "           1       0.96      0.96      0.96      1000\n",
            "           2       0.73      0.73      0.73      1000\n",
            "           3       0.83      0.85      0.84      1000\n",
            "           4       0.72      0.78      0.75      1000\n",
            "           5       0.95      0.91      0.93      1000\n",
            "           6       0.64      0.56      0.60      1000\n",
            "           7       0.91      0.94      0.92      1000\n",
            "           8       0.93      0.94      0.93      1000\n",
            "           9       0.94      0.95      0.94      1000\n",
            "\n",
            "    accuracy                           0.84     10000\n",
            "   macro avg       0.84      0.84      0.84     10000\n",
            "weighted avg       0.84      0.84      0.84     10000\n",
            "\n",
            "average accuracy: 84.16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uGsD896s1z2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2lmmR-HI1z4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise: Training a Classification Model\n",
        "\n",
        "We just learned how to do classification using Fashion MNIST, a data set containing items of clothing. There's another, similar dataset called MNIST which has images of handwriting -- specifically handwritten digits 0 through 9.\n",
        "\n",
        "*    Write an MNIST classifier that is trained to recognise the written digit. I've started the code for you below -- how would you finish it? What's the best accuracy you can achieve?"
      ],
      "metadata": {
        "id": "k9fIubWIvNxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(X_train, y_train),(X_test, y_test) = mnist.load_data()"
      ],
      "metadata": {
        "id": "BviWe77t1z6y",
        "outputId": "817a9d00-60e8-41b5-a4e3-1e3e1f95c316",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0]"
      ],
      "metadata": {
        "id": "84RkmZy1wT0M",
        "outputId": "92ddd3f7-b587-4555-f230-d0016d0830fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
              "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
              "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
              "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
              "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
              "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
              "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
              "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
              "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
              "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
              "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
              "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
              "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
              "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
              "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]], dtype=uint8)"
            ],
            "text/html": [
              "<style>\n",
              "      .ndarray_repr .ndarray_raw_data {\n",
              "        display: none;\n",
              "      }\n",
              "      .ndarray_repr.show_array .ndarray_raw_data {\n",
              "        display: block;\n",
              "      }\n",
              "      .ndarray_repr.show_array .ndarray_image_preview {\n",
              "        display: none;\n",
              "      }\n",
              "      </style>\n",
              "      <div id=\"id-ee3c1679-540a-4977-860d-fb22309f280a\" class=\"ndarray_repr\"><pre>ndarray (28, 28) <button style=\"padding: 0 2px;\">show data</button></pre><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABAElEQVR4nGNgGMyAWUhIqK5jvdSy/9/rGRgYGFhgEnJsVjYCwQwMDAxPJgV+vniQgYGBgREqZ7iXH8r6l/SV4dn7m8gmCt3++/fv37/Htn3/iMW+gDnZf/+e5WbQnoXNNXyMs/5GoQoxwVmf/n9kSGFiwAW49/11wynJoPzx4YIcRlyygR/+/i2XxCWru+vv32nSuGQFYv/83Y3b4p9/fzpAmSyoMnohpiwM1w5h06Q+5enfv39/bcMiJVF09+/fv39P+mFKiTtd/fv3799jgZiBJLT69t+/f/8eDuDEkDJf8+jv379/v7Ryo4qzMDAwMAQGMjBc3/y35wM2V1IfAABFF16Aa0wAOwAAAABJRU5ErkJggg==\" class=\"ndarray_image_preview\" /><pre class=\"ndarray_raw_data\">array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
              "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
              "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
              "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
              "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
              "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
              "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
              "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
              "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
              "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
              "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
              "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
              "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
              "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
              "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]], dtype=uint8)</pre></div><script>\n",
              "      (() => {\n",
              "      const titles = ['show data', 'hide data'];\n",
              "      let index = 0\n",
              "      document.querySelector('#id-ee3c1679-540a-4977-860d-fb22309f280a button').onclick = (e) => {\n",
              "        document.querySelector('#id-ee3c1679-540a-4977-860d-fb22309f280a').classList.toggle('show_array');\n",
              "        index = (++index) % 2;\n",
              "        document.querySelector('#id-ee3c1679-540a-4977-860d-fb22309f280a button').textContent = titles[index];\n",
              "        e.preventDefault();\n",
              "        e.stopPropagation();\n",
              "      }\n",
              "      })();\n",
              "    </script>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[0] #once again, this is the label, i.e. classification, of X_train[0]"
      ],
      "metadata": {
        "id": "ExuZ8OxK65tY",
        "outputId": "88354759-0def-4e7e-9b09-9cf9bf77b069",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nsamples, nx, ny = X_train.shape\n",
        "X_train_d2 = X_train.reshape((nsamples, nx * ny))\n",
        "nsamples, nx, ny = X_test.shape\n",
        "X_test_d2 = X_test.reshape((nsamples, nx * ny))"
      ],
      "metadata": {
        "id": "roJF4tSLxpYG"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train_d2.shape)\n",
        "print(y_train.shape)"
      ],
      "metadata": {
        "id": "is0tIPC3XUJh",
        "outputId": "82dc9dfa-e79f-4826-f941-4a032c266f76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 784)\n",
            "(60000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "lr_model = LogisticRegression(multi_class=\"multinomial\",\n",
        "                                    solver=\"saga\", max_iter=100, tol=1e-3)\n",
        "lr_model.fit(X_train_d2, y_train)\n",
        "lr_predict = lr_model.predict(X_test_d2)\n",
        "\n",
        "print(metrics.classification_report(y_test, lr_predict))\n",
        "print(\"average accuracy:\", np.mean(y_test == lr_predict) * 100)"
      ],
      "metadata": {
        "id": "VQKspXiQ8rDJ",
        "outputId": "eef2a785-34ba-490d-9275-69a57c81fcbd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.98      0.97       980\n",
            "           1       0.96      0.98      0.97      1135\n",
            "           2       0.93      0.90      0.91      1032\n",
            "           3       0.90      0.92      0.91      1010\n",
            "           4       0.94      0.93      0.94       982\n",
            "           5       0.91      0.86      0.88       892\n",
            "           6       0.95      0.95      0.95       958\n",
            "           7       0.93      0.92      0.93      1028\n",
            "           8       0.87      0.89      0.88       974\n",
            "           9       0.91      0.92      0.91      1009\n",
            "\n",
            "    accuracy                           0.93     10000\n",
            "   macro avg       0.92      0.92      0.92     10000\n",
            "weighted avg       0.93      0.93      0.93     10000\n",
            "\n",
            "average accuracy: 92.57\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "svm_model = SVC(max_iter=100, tol=1e-3)\n",
        "svm_model.fit(X_train_d2,y_train)\n",
        "svm_predict = svm_model.predict(X_test_d2)\n",
        "\n",
        "print(metrics.classification_report(y_test, svm_predict))\n",
        "print(\"average accuracy:\", np.mean(y_test == svm_predict) * 100)"
      ],
      "metadata": {
        "id": "ILcxSApQ8rDJ",
        "outputId": "8afc8ef8-3a0b-4443-81bf-0774b511960d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.99      0.98       980\n",
            "           1       0.99      0.99      0.99      1135\n",
            "           2       0.95      0.95      0.95      1032\n",
            "           3       0.93      0.93      0.93      1010\n",
            "           4       0.94      0.87      0.90       982\n",
            "           5       0.97      0.92      0.94       892\n",
            "           6       0.98      0.98      0.98       958\n",
            "           7       0.96      0.80      0.87      1028\n",
            "           8       0.92      0.94      0.93       974\n",
            "           9       0.73      0.91      0.81      1009\n",
            "\n",
            "    accuracy                           0.93     10000\n",
            "   macro avg       0.93      0.93      0.93     10000\n",
            "weighted avg       0.93      0.93      0.93     10000\n",
            "\n",
            "average accuracy: 92.71000000000001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "MLP_model = MLPClassifier(max_iter=5000, tol=1e-8) #choose eour model , set the parameters\n",
        "MLP_model.fit(X_train_d2,y_train)    #train the model\n",
        "mlp_predict = MLP_model.predict(X_test_d2)   #try to predict\n",
        "\n",
        "print(metrics.classification_report(y_test, mlp_predict))\n",
        "print(\"average accuracy:\", np.mean(y_test == mlp_predict) * 100)"
      ],
      "metadata": {
        "id": "sh4bkJ5o9Ish",
        "outputId": "7faf1b18-452c-4431-dde7-280e158ad04e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.99      0.97       980\n",
            "           1       0.98      0.98      0.98      1135\n",
            "           2       0.97      0.95      0.96      1032\n",
            "           3       0.95      0.95      0.95      1010\n",
            "           4       0.98      0.96      0.97       982\n",
            "           5       0.95      0.94      0.94       892\n",
            "           6       0.98      0.97      0.98       958\n",
            "           7       0.98      0.96      0.97      1028\n",
            "           8       0.94      0.96      0.95       974\n",
            "           9       0.96      0.96      0.96      1009\n",
            "\n",
            "    accuracy                           0.96     10000\n",
            "   macro avg       0.96      0.96      0.96     10000\n",
            "weighted avg       0.96      0.96      0.96     10000\n",
            "\n",
            "average accuracy: 96.44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "gNB_model = GaussianNB()\n",
        "gNB_model.fit(X_train_d2,y_train)\n",
        "nb_predict = gNB_model.predict(X_test_d2)\n",
        "\n",
        "print(metrics.classification_report(y_test, nb_predict))\n",
        "print(\"average accuracy:\", np.mean(y_test == nb_predict) * 100)"
      ],
      "metadata": {
        "id": "bNndiutM9Isi",
        "outputId": "44778317-3ecd-4414-e06c-d2624aa44b2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.89      0.84       980\n",
            "           1       0.85      0.95      0.90      1135\n",
            "           2       0.90      0.26      0.40      1032\n",
            "           3       0.71      0.35      0.47      1010\n",
            "           4       0.88      0.17      0.29       982\n",
            "           5       0.55      0.05      0.09       892\n",
            "           6       0.65      0.93      0.77       958\n",
            "           7       0.88      0.27      0.42      1028\n",
            "           8       0.28      0.67      0.40       974\n",
            "           9       0.37      0.95      0.53      1009\n",
            "\n",
            "    accuracy                           0.56     10000\n",
            "   macro avg       0.69      0.55      0.51     10000\n",
            "weighted avg       0.69      0.56      0.52     10000\n",
            "\n",
            "average accuracy: 55.58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import SGDRegressor\n",
        "\n",
        "sgdr_model = SGDRegressor(max_iter=500, tol=1e-3)\n",
        "sgdr_model.fit(X_train_d2,y_train)\n",
        "sgdr_predict = sgdr_model.predict(X_test_d2)\n",
        "\n",
        "print(metrics.classification_report(y_test, sgdr_predict))\n",
        "print(\"average accuracy:\", np.mean(y_test == sgdr_predict) * 100)"
      ],
      "metadata": {
        "id": "7b-qxajA0FmX",
        "outputId": "63f03b7f-58c3-420c-e956-7ba6872b8098",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Classification metrics can't handle a mix of multiclass and continuous targets",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-7904a1ba16f7>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msgdr_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msgdr_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_d2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgdr_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"average accuracy:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0msgdr_predict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   2308\u001b[0m     \"\"\"\n\u001b[1;32m   2309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2310\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2312\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m     96\u001b[0m             \"Classification metrics can't handle a mix of {0} and {1} targets\".format(\n\u001b[1;32m     97\u001b[0m                 \u001b[0mtype_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of multiclass and continuous targets"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dAuDoH0z0FrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Uy4avYks0FuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wi9Aa7Cj10Bl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}